# ðŸš€ Inference-in-a-Box: Enterprise AI/ML Platform Demo

A complete, production-ready inference platform that demonstrates enterprise-grade AI/ML model serving using modern cloud-native technologies. This platform combines **Istio service mesh**, **KServe serverless model serving**, and **comprehensive observability** to create a robust, scalable, and secure inference-as-a-service solution.

## ðŸŽ¯ What You're Building

**Inference-in-a-Box** is a comprehensive demonstration of how modern organizations can deploy AI/ML models at enterprise scale with:

- **ðŸ”’ Zero-Trust Security** - Automatic mTLS encryption, fine-grained authorization, and compliance-ready audit logging
- **âš¡ Serverless Inference** - Auto-scaling from zero to N instances based on traffic demand
- **ðŸŒ Multi-Tenant Architecture** - Secure isolation between different teams, projects, and customers
- **ðŸ“Š Enterprise Observability** - Full-stack monitoring, distributed tracing, and AI-specific metrics
- **ðŸšª Unified AI Gateway** - Single entry point for multiple AI providers and models
- **ðŸŽ›ï¸ Traffic Management** - Canary deployments, A/B testing, and intelligent routing

## ðŸ—ï¸ Platform Architecture

```mermaid
graph TB
    subgraph "Inference-in-a-Box Cluster"
        subgraph "Observability Stack"
            P[Prometheus]
            G[Grafana]
            J[Jaeger]
            K[Kiali]
            AM[AlertManager]
        end
        
        subgraph "Istio Service Mesh"
            IC[Istiod]
            IG[Istio Gateway]
        end
        
        subgraph "AI Gateway Layer"
            EAG[Envoy AI Gateway]
            RLM[Rate Limiting]
            AUTH[Authentication]
        end
        
        subgraph "Multi-Tenant Namespaces"
            subgraph "Tenant A"
                KS1[KServe Models]
                IS1[Istio Sidecar]
            end
            subgraph "Tenant B"
                KS2[KServe Models]
                IS2[Istio Sidecar]
            end
            subgraph "Tenant C"
                KS3[KServe Models]
                IS3[Istio Sidecar]
            end
        end
        
        subgraph "KServe Infrastructure"
            KC[KServe Controller]
            KN[Knative Serving]
            CM[Cert Manager]
        end
    end
    
    subgraph "External"
        CLIENT[AI Client Apps]
        MODELS[Model Registry]
    end
    
    CLIENT --> IG
    IG --> EAG
    EAG --> AUTH
    AUTH --> RLM
    RLM --> KS1
    RLM --> KS2
    RLM --> KS3
    
    P --> IC
    G --> P
    J --> IS1
    J --> IS2
    J --> IS3
    
    MODELS --> KS1
    MODELS --> KS2
    MODELS --> KS3
    
    KC --> KN
    KN --> KS1
    KN --> KS2
    KN --> KS3
```

## ðŸ› ï¸ Technology Stack

### Core Platform Components
- **ðŸ³ Kind** - Local Kubernetes cluster for development and testing
- **ðŸ•¸ï¸ Istio** - Service mesh for security, traffic management, and observability
- **ðŸ¤– KServe** - Kubernetes-native model serving with auto-scaling
- **ðŸŒŠ Knative** - Serverless framework for event-driven applications
- **ðŸ” Cert Manager** - Automated certificate management

### Observability & Monitoring
- **ðŸ“ˆ Prometheus** - Metrics collection and alerting
- **ðŸ“Š Grafana** - Visualization and dashboards
- **ðŸ” Jaeger** - Distributed tracing
- **ðŸ—ºï¸ Kiali** - Service mesh visualization
- **ðŸš¨ AlertManager** - Alert routing and management

### AI/ML Support
- **ðŸ§  TensorFlow Serving** - TensorFlow model serving
- **ðŸ”¥ PyTorch Serve** - PyTorch model serving  
- **âš¡ Scikit-learn** - Traditional ML model serving
- **ðŸ¤— Hugging Face** - Transformer model support

## ðŸŽ¯ Key Features Demonstrated

### ðŸ”’ Enterprise Security
```mermaid
sequenceDiagram
    participant Client
    participant Gateway as Envoy AI Gateway
    participant Auth as Authentication
    participant Istio as Istio Proxy
    participant Model as KServe Model
    
    Client->>Gateway: Inference Request
    Gateway->>Auth: Validate JWT/API Key
    Auth-->>Gateway: Authentication Result
    Gateway->>Istio: Forward Request (mTLS)
    Istio->>Model: Secure Request
    Model-->>Istio: Inference Response
    Istio-->>Gateway: Secure Response
    Gateway-->>Client: Final Response
    
    Note over Gateway,Model: All communication encrypted with mTLS
    Note over Auth: RBAC policies enforced
    Note over Istio: Zero-trust networking
```

- **Zero-trust networking** with automatic mTLS between all services
- **Multi-tenant isolation** with namespace-based security boundaries
- **RBAC and authentication** with JWT/API key validation
- **Audit logging** for compliance requirements (GDPR, HIPAA, SOC 2)
- **Certificate management** with automatic rotation

### âš¡ AI/ML Model Serving
```mermaid
graph LR
    subgraph "Model Lifecycle"
        MR[Model Registry] --> KS[KServe Controller]
        KS --> KN[Knative Serving]
        KN --> POD[Model Pods]
    end
    
    subgraph "Auto-scaling"
        POD --> AS[Auto-scaler]
        AS --> |Scale Up| POD
        AS --> |Scale to Zero| ZERO[No Pods]
        ZERO --> |Cold Start| POD
    end
    
    subgraph "Traffic Management"
        CANARY[Canary Deploy]
        AB[A/B Testing]
        BLUE[Blue/Green]
    end
    
    POD --> CANARY
    POD --> AB
    POD --> BLUE
```

- **Serverless auto-scaling** from zero to N instances based on demand
- **Multi-framework support** (TensorFlow, PyTorch, Scikit-learn, Hugging Face)
- **Canary deployments** for gradual model rollouts
- **A/B testing** with intelligent traffic splitting
- **Model versioning** and rollback capabilities
- **Resource optimization** with GPU/CPU scheduling

### ðŸŒ Multi-Tenancy & Governance
- **Workspace isolation** with dedicated namespaces per tenant
- **Resource quotas** and governance policies
- **Separate observability** scopes for each tenant
- **Independent lifecycle** management and deployment schedules
- **Cost tracking** and chargeback mechanisms

### ðŸ“Š Comprehensive Observability
```mermaid
graph TB
    subgraph "Metrics Pipeline"
        ISTIO[Istio Metrics] --> PROM[Prometheus]
        KSERVE[KServe Metrics] --> PROM
        CUSTOM[Custom AI Metrics] --> PROM
        PROM --> GRAF[Grafana Dashboards]
    end
    
    subgraph "Tracing Pipeline"
        REQ[Request Traces] --> JAEGER[Jaeger]
        SPANS[Service Spans] --> JAEGER
        JAEGER --> ANALYSIS[Trace Analysis]
    end
    
    subgraph "Logging Pipeline"
        LOGS[Application Logs] --> LOKI[Loki]
        AUDIT[Audit Logs] --> LOKI
        LOKI --> GRAF
    end
    
    subgraph "Alerting"
        PROM --> ALERT[AlertManager]
        ALERT --> SLACK[Slack/Email]
    end
```

- **End-to-end distributed tracing** across the entire inference pipeline
- **AI-specific metrics** including inference latency, throughput, and accuracy
- **Business metrics** for cost optimization and resource planning
- **SLA monitoring** with automated alerting
- **Unified dashboards** for operational visibility

## ðŸš€ Quick Start

### Prerequisites
Ensure you have the following tools installed:
```bash
# Required tools
docker --version          # Docker 20.10+
kind --version           # Kind 0.20+
kubectl version --client  # kubectl 1.24+
helm version             # Helm 3.12+
curl --version           # curl (any recent version)
jq --version             # jq 1.6+
```

### One-Command Bootstrap
```bash
# Clone the repository
git clone <repository-url>
cd inference-in-a-box

# Bootstrap the entire platform (takes 10-15 minutes)
./scripts/bootstrap.sh

# Run demo scenarios
./scripts/demo.sh

# Access the platform
echo "ðŸŽ‰ Platform is ready!"
echo "ðŸ“Š Grafana: http://localhost:3000 (admin/admin)"
echo "ðŸ” Jaeger: http://localhost:16686"
echo "ðŸ“ˆ Prometheus: http://localhost:9090"
echo "ðŸ—ºï¸ Kiali: http://localhost:20001"
echo "ðŸ¤– Models: http://localhost:8080"
```

### Step-by-Step Setup
```bash
# 1. Create Kind cluster
./scripts/clusters/create-kind-cluster.sh

# 2. Install core infrastructure
./scripts/install/install-istio.sh
./scripts/install/install-kserve.sh
./scripts/install/install-observability.sh

# 3. Deploy sample models
./scripts/models/deploy-samples.sh

# 4. Configure security and policies
./scripts/security/setup-policies.sh

# 5. Run tests
./scripts/test/run-tests.sh
```

## Features Demonstrated

### ðŸ”’ Enterprise Security
- Zero-trust networking with automatic mTLS
- Multi-tenant isolation with workspace boundaries
- RBAC and authentication policies
- Certificate management and rotation

### ðŸŽ¯ AI/ML Model Serving
- Multiple ML frameworks (TensorFlow, PyTorch, Scikit-learn)
- Auto-scaling from zero to N instances
- Canary deployments and A/B testing
- Model versioning and rollback

### ðŸŒ Traffic Management
- Intelligent routing and load balancing
- Circuit breaking and failover
- Rate limiting and throttling
- Geographic routing simulation

### ðŸ“Š Observability
- Distributed tracing across the inference pipeline
- Custom metrics for AI workloads
- Unified logging and monitoring
- SLA tracking and alerting

### ðŸ¢ Multi-Tenancy
- Namespace-based tenant isolation
- Resource quotas and governance
- Separate observability scopes
- Independent lifecycle management

## Directory Structure

```
inference-in-a-box/
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bootstrap.sh
â”‚   â”œâ”€â”€ cleanup.sh
â”‚   â”œâ”€â”€ demo.sh
â”‚   â””â”€â”€ clusters/
â”‚       â”œâ”€â”€ create-kind-cluster.sh
â”‚       â””â”€â”€ setup-networking.sh
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ clusters/
â”‚   â”‚   â””â”€â”€ cluster.yaml
â”‚   â”œâ”€â”€ istio/
â”‚   â”‚   â”œâ”€â”€ installation.yaml
â”‚   â”‚   â”œâ”€â”€ gateway.yaml
â”‚   â”‚   â””â”€â”€ virtual-services/
â”‚   â”œâ”€â”€ kserve/
â”‚   â”‚   â”œâ”€â”€ installation.yaml
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”œâ”€â”€ envoy-ai-gateway/
â”‚   â”‚   â””â”€â”€ configuration.yaml
â”‚   â””â”€â”€ observability/
â”‚       â”œâ”€â”€ prometheus.yaml
â”‚       â”œâ”€â”€ jaeger.yaml
â”‚       â””â”€â”€ grafana/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sklearn-iris/
â”‚   â”œâ”€â”€ tensorflow-mnist/
â”‚   â””â”€â”€ pytorch-resnet/
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ inference-requests/
â”‚   â”œâ”€â”€ security-policies/
â”‚   â””â”€â”€ traffic-scenarios/
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ deployment-guide.md
    â””â”€â”€ troubleshooting.md
```

## Prerequisites

- Docker Desktop or equivalent
- kubectl
- kind
- helm
- curl
- jq

## ðŸŽ­ Demo Scenarios

### 1. ðŸ”’ Security & Authentication Demo
```mermaid
sequenceDiagram
    participant User
    participant Gateway
    participant Auth
    participant Model
    
    User->>Gateway: Request with JWT
    Gateway->>Auth: Validate Token
    Auth-->>Gateway: Authorized
    Gateway->>Model: Forward Request (mTLS)
    Model-->>Gateway: Inference Result
    Gateway-->>User: Secure Response
```

### 2. âš¡ Auto-scaling Demo
```bash
# Generate load to trigger auto-scaling
./scripts/demo/load-test.sh --model sklearn-iris --duration 300s

# Watch pods scale from 0 to N
watch "kubectl get pods -n tenant-a -l serving.kserve.io/inferenceservice=sklearn-iris"
```

### 3. ðŸš¦ Canary Deployment Demo
```bash
# Deploy new model version with canary
./scripts/demo/canary-deployment.sh --model tensorflow-mnist --version v2 --traffic 10%

# Monitor traffic split
kubectl get virtualservice -n tenant-a
```

### 4. ðŸŒ Multi-Tenant Isolation Demo
```bash
# Deploy same model to different tenants
./scripts/demo/multi-tenant.sh

# Verify isolation
kubectl get networkpolicies -A
```

## ðŸ“Š Monitoring & Observability

### Real-time Dashboards
```mermaid
graph LR
    subgraph "Grafana Dashboards"
        OVERVIEW["ðŸ“Š Platform Overview"]
        MODELS["ðŸ¤– Model Performance"]
        SECURITY["ðŸ”’ Security Metrics"]
        BUSINESS["ðŸ’° Business KPIs"]
    end
    
    subgraph "Data Sources"
        PROM["ðŸ“ˆ Prometheus"]
        JAEGER["ðŸ” Jaeger"]
        ISTIO["ðŸ•¸ï¸ Istio Metrics"]
        KSERVE["ðŸ¤– KServe Metrics"]
    end
    
    PROM --> OVERVIEW
    PROM --> MODELS
    ISTIO --> SECURITY
    KSERVE --> BUSINESS
    JAEGER --> MODELS
```

### Key Metrics Tracked
- **ðŸŽ¯ Model Performance**: Inference latency, throughput, accuracy
- **âš¡ Infrastructure**: CPU/Memory usage, auto-scaling events
- **ðŸ”’ Security**: Authentication failures, policy violations
- **ðŸ’° Business**: Cost per inference, tenant usage, SLA compliance
- **ðŸŒ Network**: Request rates, error rates, circuit breaker events

### Alert Configuration
```yaml
# Example alert rules
groups:
- name: inference.rules
  rules:
  - alert: HighInferenceLatency
    expr: histogram_quantile(0.95, rate(kserve_request_duration_seconds_bucket[5m])) > 1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High inference latency detected"
      
  - alert: ModelDown
    expr: up{job="kserve-model"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Model service is down"
```

## Getting Started

See [docs/deployment-guide.md](docs/deployment-guide.md) for detailed setup instructions.

## Troubleshooting

See [docs/troubleshooting.md](docs/troubleshooting.md) for common issues and solutions.